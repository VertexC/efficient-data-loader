{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc39c7",
   "metadata": {},
   "source": [
    "# Performance Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd53272",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.researchgate.net/publication/301463871_HPDBSCAN_highly_parallel_DBSCAN\n",
    "    \n",
    "eps is set to 0.01, and minPoints as 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ea74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13fd24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['Clusters', 'DBSCAN']>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('./data/twitterSmall.h5.h5', 'r')\n",
    "print(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72b5a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3704351\n",
      "5820\n"
     ]
    }
   ],
   "source": [
    "print(len(f['Clusters']))\n",
    "print(len(set(f['Clusters'][:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6b617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53.544956 -2.767532]\n",
      " [51.623474 -0.134995]\n",
      " [51.948055 -2.069167]\n",
      " [51.080994 -1.331917]\n",
      " [52.204224 -2.230113]\n",
      " [51.60577  -0.347001]\n",
      " [53.579166  0.654444]\n",
      " [51.11222  -0.163057]\n",
      " [51.52723  -0.107052]\n",
      " [52.403286 -1.479962]]\n"
     ]
    }
   ],
   "source": [
    "print(f['DBSCAN'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7913333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "dataset = f['DBSCAN']\n",
    "for size in [20000, 50000, 100000]:\n",
    "    rng = default_rng()\n",
    "    numbers = rng.choice(len(dataset), size=size, replace=False)\n",
    "    X = np.asarray(dataset)[numbers]\n",
    "    with open(\"./data/benchmark/twitter_{}.txt\".format(len(X)), \"w\") as f_o:\n",
    "        # Writing data to a file\n",
    "        f_o.write(\"{} \\n\".format(len(X)))\n",
    "        f_o.write(\"{} {}\\n\".format(0.01, 40))\n",
    "        for i in range(len(X)):\n",
    "            f_o.write(\"{} {}\\n\".format(X[i ,0], X[i ,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc2392",
   "metadata": {},
   "source": [
    "# Correctness Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23db74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(f['DBSCAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be3b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1512x936 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 20,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 20,\n",
    "            \"xi\": 0.25,\n",
    "        },\n",
    "    ),\n",
    "    (noisy_moons, {\"damping\": 0.75, \"preference\": -220, \"n_clusters\": 2}),\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 5,\n",
    "            \"xi\": 0.035,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 20,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "    \n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "   \n",
    "    with open(\"./data/simple/simple_{}_1500.txt\".format(i_dataset), \"w\") as f:\n",
    "        # Writing data to a file\n",
    "        f.write(\"{} \\n\".format(len(X)))\n",
    "        f.write(\"{} {}\\n\".format(params[\"eps\"], 5))\n",
    "        for i in range(len(X)):\n",
    "            f.write(\"{} {}\\n\".format(X[i ,0], X[i ,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67ddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
